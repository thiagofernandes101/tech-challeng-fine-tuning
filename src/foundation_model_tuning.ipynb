{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "606315ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thiagofernandes101/projects/fiap/FineTunningTechChallenge/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_84805/4069057207.py:6: UserWarning: WARNING: Unsloth should be imported before transformers to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import FastLanguageModel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.9.5: Fast Llama patching. Transformers: 4.56.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060 Laptop GPU. Num GPUs = 1. Max memory: 6.0 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.9.5 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA dispon√≠vel? True\n",
      "N√∫mero de GPUs dispon√≠veis: 1\n",
      "GPU 0: NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "## ler as propriedades title e content do arquivo trn.json para o finetuning do modelo\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import FastLanguageModel\n",
    "from trl import SFTTrainer\n",
    "from datasets import Dataset\n",
    "\n",
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "\n",
    "fourbit_models = [\n",
    "    #\"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
    "    #\"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
    "    #\"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    #\"unsloth/gemma-2-9b-bnb-4bit\"\n",
    "]\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Phi-3.5-mini-instruct\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")\n",
    "\n",
    "def solve_relative_path(file_path: str) -> str:\n",
    "    project_root: str = os.path.dirname(os.getcwd())\n",
    "    data_path: str = os.path.abspath(os.path.join(project_root, file_path))\n",
    "    return data_path\n",
    "\n",
    "def read_json_file(file_path: str) -> list[tuple[str, str]]:\n",
    "    training_data = []\n",
    "\n",
    "    with open(file_path, \"r\") as file:\n",
    "        buffer: str = \"\" \n",
    "        for line in file:\n",
    "            buffer += line.strip()\n",
    "            try:\n",
    "                item: dict = json.loads(buffer)\n",
    "                buffer = \"\"\n",
    "                title: str = item.get(\"title\", \"\")\n",
    "                content: str = item.get(\"content\", \"\")\n",
    "                if title and content:\n",
    "                    training_data.append((title, content))\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    return training_data\n",
    "\n",
    "def verify_gpu_availability() -> None:\n",
    "    is_cuda_available = torch.cuda.is_available()\n",
    "    print(f\"CUDA dispon√≠vel? {is_cuda_available}\")\n",
    "\n",
    "    if is_cuda_available:\n",
    "        gpu_count = torch.cuda.device_count()\n",
    "        print(f\"N√∫mero de GPUs dispon√≠veis: {gpu_count}\")\n",
    "        for i in range(gpu_count):\n",
    "            print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "def format_data_for_training_with_phi35(data: list[tuple[str,str]]) -> list[str]:\n",
    "    formatted_data = []\n",
    "    for title, content in data:\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": f\"Tell me about the following title in 10 words: {title}\"},\n",
    "            {\"role\": \"assistant\", \"content\": content},\n",
    "        ]\n",
    "        formatted_data.append({\"text\": tokenizer.apply_chat_template(messages, tokenize=False)})\n",
    "    return formatted_data\n",
    "\n",
    "verify_gpu_availability()\n",
    "\n",
    "raw_data = read_json_file(solve_relative_path(\"datasets/trn.json\"))\n",
    "formatted_data = format_data_for_training_with_phi35(raw_data)\n",
    "dataset = Dataset.from_dict({\"text\": [item[\"text\"] for item in formatted_data]})\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"/home/thiagofernandes101/projects/fiap/FineTunningTechChallenge/results\", # Pasta para salvar os resultados\n",
    "    per_device_train_batch_size = 2, # Tamanho do lote. Use 1 ou 2 se tiver pouca VRAM.\n",
    "    gradient_accumulation_steps = 4, # Acumula gradientes para simular um lote maior (2 * 4 = lote efetivo de 8)\n",
    "    warmup_steps = 5, # Passos de aquecimento para o otimizador\n",
    "    max_steps = 60, # N√∫mero total de passos de treinamento. Ajuste conforme necess√°rio.\n",
    "    learning_rate = 2e-4, # Taxa de aprendizado\n",
    "    fp16 = not torch.cuda.is_bf16_supported(), # Usa precis√£o de 16 bits\n",
    "    bf16 = torch.cuda.is_bf16_supported(),\n",
    "    logging_steps = 1, # A cada quantos passos ele mostra o progresso (loss)\n",
    "    optim = \"adamw_8bit\", # Otimizador eficiente em mem√≥ria\n",
    "    weight_decay = 0.01,\n",
    "    lr_scheduler_type = \"linear\",\n",
    "    seed = 42,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    args = training_args,\n",
    ")\n",
    "\n",
    "print(\"Iniciando o fine-tuning...\")\n",
    "trainer.train()\n",
    "print(\"Fine-tuning conclu√≠do!\")\n",
    "\n",
    "\n",
    "title_for_testing = \"A Day in the Life of China\"\n",
    "message_for_testing = [\n",
    "    {\"role\": \"user\", \"content\": f\"Tell me about the following title in 10 words: {title_for_testing}\"},\n",
    "]\n",
    "input_ids = tokenizer.apply_chat_template(message_for_testing, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(input_ids=input_ids, max_new_tokens=256, use_cache=True)\n",
    "results = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "\n",
    "print(\"\\n--- TESTE DE INFER√äNCIA ---\")\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
