{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import os, re\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    import torch; v = re.match(r\"[0-9\\.]{3,}\", str(torch.__version__)).group(0)\n",
        "    xformers = \"xformers==\" + (\"0.0.32.post2\" if v == \"2.8.0\" else \"0.0.29.post3\")\n",
        "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "!pip install transformers==4.55.4\n",
        "!pip install --no-deps trl==0.22.2"
      ],
      "metadata": {
        "id": "mh0kET2MHg2S"
      },
      "id": "mh0kET2MHg2S",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "DAXn5EuAHdNW"
      },
      "id": "DAXn5EuAHdNW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b78a0772",
      "metadata": {
        "id": "b78a0772"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from datasets import Dataset, Features, Value\n",
        "import json\n",
        "import os\n",
        "import torch\n",
        "import re\n",
        "import html\n",
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03a14c4d",
      "metadata": {
        "id": "03a14c4d"
      },
      "outputs": [],
      "source": [
        "max_seq_length = 1024\n",
        "dtype = None\n",
        "load_in_4bit = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9deaa167",
      "metadata": {
        "id": "9deaa167"
      },
      "outputs": [],
      "source": [
        "def clean_text(text: str):\n",
        "    text = html.unescape(text)\n",
        "    text = re.sub(r\"--.*\", \"\", text) # Remove lines starting with '--' or similar patterns\n",
        "    text = re.sub(r\"[^\\x00-\\x7F]+\", \"\", text) # Remove non-ASCII characters (optional, depending on your dataset)\n",
        "    text = re.sub(r\"\\s+\", \" \", text) # Remove excessive whitespace\n",
        "    text = text.strip()\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a98619e",
      "metadata": {
        "id": "9a98619e"
      },
      "outputs": [],
      "source": [
        "def read_json_file(file_path: str):\n",
        "    training_data = []\n",
        "\n",
        "    with open(file_path, \"r\") as file:\n",
        "        buffer: str = \"\"\n",
        "        for line in file:\n",
        "            buffer += line.strip()\n",
        "            try:\n",
        "                item: dict = json.loads(buffer)\n",
        "                buffer: str = \"\"\n",
        "                title: str = clean_text(item.get(\"title\", \"\"))\n",
        "                content: str = clean_text(item.get(\"content\", \"\"))\n",
        "                if title and content:\n",
        "                    yield {\"title\": title, \"content\": content}\n",
        "            except json.JSONDecodeError:\n",
        "                continue\n",
        "    return training_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb63a54e",
      "metadata": {
        "id": "bb63a54e"
      },
      "outputs": [],
      "source": [
        "def format_for_training(example: dict, tokenizer) -> dict:\n",
        "    \"\"\"\n",
        "    Formats a title and content into a chat-like format using the tokenizer's chat template.\n",
        "    This teaches the model to respond with the book's content when asked about its title.\n",
        "    \"\"\"\n",
        "    conversation = [\n",
        "        {'role': 'user', 'content': f'Tell me about the book titled \"{example[\"title\"]}\". What is its content?'},\n",
        "        {'role': 'assistant', 'content': example['content']}\n",
        "    ]\n",
        "    formatted_text = tokenizer.apply_chat_template(conversation, tokenize=False, add_generation_prompt=False)\n",
        "    return {\"text\": formatted_text + tokenizer.eos_token}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bab92bb",
      "metadata": {
        "id": "7bab92bb"
      },
      "outputs": [],
      "source": [
        "fourbit_models = [\n",
        "    #\"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
        "    #\"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
        "    #\"unsloth/Phi-3-medium-4k-instruct\",\n",
        "    #\"unsloth/gemma-2-9b-bnb-4bit\"\n",
        "]\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Phi-3.5-mini-instruct\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                    \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1652951",
      "metadata": {
        "id": "b1652951"
      },
      "outputs": [],
      "source": [
        "dataset_path = \"/content/drive/MyDrive/FineTuningTechChallenge/trn.json\"\n",
        "\n",
        "raw_data = read_json_file(dataset_path)\n",
        "training_data_list = [{\"title\": item[\"title\"], \"content\": item[\"content\"]} for item in raw_data]\n",
        "base_dataset = Dataset.from_list(training_data_list)\n",
        "dataset = base_dataset.map(lambda x: format_for_training(x, tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bdb4d83",
      "metadata": {
        "id": "0bdb4d83"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/results\",\n",
        "    per_device_train_batch_size=2,  # Aumente se sua GPU permitir para acelerar\n",
        "    gradient_accumulation_steps=4, # Ajuste para manter um lote efetivo de 8 (2*4)\n",
        "    warmup_steps=10,\n",
        "    # max_steps = 70, # Remova esta linha\n",
        "    num_train_epochs=1, # Adicione esta linha para treinar em todo o dataset uma vez\n",
        "    learning_rate=2e-4,\n",
        "    fp16=not torch.cuda.is_bf16_supported(),\n",
        "    bf16=torch.cuda.is_bf16_supported(),\n",
        "    logging_steps=1,\n",
        "    optim=\"adamw_8bit\",\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    seed=42,\n",
        "    save_strategy=\"epoch\", # Salvar ao final de cada época\n",
        "    save_total_limit=2,\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    args = training_args,\n",
        "    packing = True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cfe6f2a",
      "metadata": {
        "id": "8cfe6f2a"
      },
      "outputs": [],
      "source": [
        "title_for_testing = \"A Day in the Life of China\"\n",
        "message_for_testing = [\n",
        "    {\"role\": \"user\", \"content\": f\"Regarding the book {title_for_testing}, what was the author’s primary goal in writing it?\"},\n",
        "]\n",
        "input_ids = tokenizer.apply_chat_template(message_for_testing, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model.generate(input_ids=input_ids, max_new_tokens=256, use_cache=True)\n",
        "results = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "\n",
        "print(\"\\n--- TESTE ANTES DO TREINO ---\")\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22d2414a",
      "metadata": {
        "id": "22d2414a"
      },
      "outputs": [],
      "source": [
        "print(\"Iniciando o fine-tuning...\")\n",
        "trainer.train()\n",
        "print(\"Fine-tuning concluído!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "767e6e7a",
      "metadata": {
        "id": "767e6e7a"
      },
      "outputs": [],
      "source": [
        "# del model\n",
        "# del tokenizer\n",
        "# del trainer\n",
        "# del dataset\n",
        "\n",
        "# gc.collect()\n",
        "# torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "606315ab",
      "metadata": {
        "id": "606315ab"
      },
      "outputs": [],
      "source": [
        "title_for_testing = \"A Day in the Life of China\"\n",
        "message_for_testing = [\n",
        "    {\"role\": \"user\", \"content\": f\"Regarding the book {title_for_testing}, what was the author’s primary goal in writing it?\"},\n",
        "]\n",
        "input_ids = tokenizer.apply_chat_template(message_for_testing, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model.generate(input_ids=input_ids, max_new_tokens=256, use_cache=True)\n",
        "results = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "\n",
        "print(\"\\n--- TESTE DE INFERÊNCIA ---\")\n",
        "print(results)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}