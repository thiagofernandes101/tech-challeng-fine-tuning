{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "606315ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thiagofernandes101/projects/fiap/FineTunningTechChallenge/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.9.5: Fast Llama patching. Transformers: 4.56.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060 Laptop GPU. Num GPUs = 1. Max memory: 6.0 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.9.5 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA dispon√≠vel? True\n",
      "N√∫mero de GPUs dispon√≠veis: 1\n",
      "GPU 0: NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 1389915 examples [01:45, 13164.98 examples/s]\n",
      "Unsloth: Tokenizing [\"text\"] (num_proc=20): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1389915/1389915 [01:18<00:00, 17812.71 examples/s]\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 32009}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando o fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 1,389,915 | Num Epochs = 1 | Total steps = 60\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 8\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 8 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 29,884,416 of 3,850,963,968 (0.78% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 04:39, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.416800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.309700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.116600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.924200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.157800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.206600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.708200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.696500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.642600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.676200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.553600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.607800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.859900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.420000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.311900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.403300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.227000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.457600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.150700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.234400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>2.735100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>2.275700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>2.093900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>2.322100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.279300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>2.435400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>2.286200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>2.056300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>2.524400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.177400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>2.155800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>2.438100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>2.233800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>2.021100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>2.231300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>2.433700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>2.225200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>2.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>2.438000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.223800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>2.151700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.885300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>2.124500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>2.246200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>2.404300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>2.269200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>2.295200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>2.307800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>2.209700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.414600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>2.399600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>2.168000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>2.498200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>2.285700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>2.129200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>2.256800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>2.250800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>2.351100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>2.406600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.929200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning conclu√≠do!\n",
      "\n",
      "--- TESTE DE INFER√äNCIA ---\n",
      "Tell me about the following title in 10 words: A Day in the Life of China A Day in the Life of China is a book that will make you think about the world in a new way. It is a book that will make you think about the world in a new way. It is a book that will make you think about the world in a new way. It is a book that will make you think about the world in a new way. It is a book that will make you think about the world in a new way. It is a book that will make you think about the world in a new way. It is a book that will make you think about the world in a new way. It is a book that will make you think about the world in a new way. It is a book that will make you think about the world in a new way. It is a book that will make you think about the world in a new way. It is a book that will make you think about the world in a new way. It is a book that will make you think about the world in a new way. It is a book that will make you think about the world in a new way. It is a book that will make you think about the world in a new way. It is a book that will make you think about the world\n"
     ]
    }
   ],
   "source": [
    "## ler as propriedades title e content do arquivo trn.json para o finetuning do modelo\n",
    "from unsloth import FastLanguageModel\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from datasets import Dataset\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import re\n",
    "import html\n",
    "\n",
    "max_seq_length = 1024\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "\n",
    "fourbit_models = [\n",
    "    #\"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
    "    #\"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
    "    #\"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    #\"unsloth/gemma-2-9b-bnb-4bit\"\n",
    "]\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Phi-3.5-mini-instruct\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,\n",
    "    loftq_config = None,\n",
    ")\n",
    "\n",
    "def solve_relative_path(file_path: str) -> str:\n",
    "    project_root: str = os.path.dirname(os.getcwd())\n",
    "    data_path: str = os.path.abspath(os.path.join(project_root, file_path))\n",
    "    return data_path\n",
    "\n",
    "def read_json_file(file_path: str) -> list[tuple[str, str]]:\n",
    "    training_data = []\n",
    "\n",
    "    with open(file_path, \"r\") as file:\n",
    "        buffer: str = \"\" \n",
    "        for line in file:\n",
    "            buffer += line.strip()\n",
    "            try:\n",
    "                item: dict = json.loads(buffer)\n",
    "                buffer = \"\"\n",
    "                title: str = clean_text(item.get(\"title\", \"\"))\n",
    "                content: str = clean_text(item.get(\"content\", \"\"))\n",
    "                if title and content:\n",
    "                    training_data.append((title, content))\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    return training_data\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    text = html.unescape(text)\n",
    "    text = re.sub(r\"--.*\", \"\", text) # Remove lines starting with '--' or similar patterns\n",
    "    text = re.sub(r\"[^\\x00-\\x7F]+\", \"\", text) # Remove non-ASCII characters (optional, depending on your dataset)\n",
    "    text = re.sub(r\"\\s+\", \" \", text) # Remove excessive whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def verify_gpu_availability() -> None:\n",
    "    is_cuda_available = torch.cuda.is_available()\n",
    "    print(f\"CUDA dispon√≠vel? {is_cuda_available}\")\n",
    "\n",
    "    if is_cuda_available:\n",
    "        gpu_count = torch.cuda.device_count()\n",
    "        print(f\"N√∫mero de GPUs dispon√≠veis: {gpu_count}\")\n",
    "        for i in range(gpu_count):\n",
    "            print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "def format_data_for_training_with_phi35(data: list[tuple[str,str]]) -> list[str]:\n",
    "    formatted_data = []\n",
    "    for title, content in data:\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": f\"Tell me about the following title in 10 words: {title}\"},\n",
    "            {\"role\": \"assistant\", \"content\": content},\n",
    "        ]\n",
    "        formatted_data.append({\"text\": tokenizer.apply_chat_template(messages, tokenize=False)})\n",
    "    return formatted_data\n",
    "\n",
    "def format_data_generator(file_path: str):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        buffer = \"\"\n",
    "        for line in f:\n",
    "            buffer += line.strip()\n",
    "            try:\n",
    "                item = json.loads(buffer)\n",
    "                buffer = \"\"\n",
    "                title = clean_text(item.get(\"title\", \"\"))\n",
    "                content = clean_text(item.get(\"content\", \"\"))\n",
    "                if title and content:\n",
    "                    messages = [\n",
    "                        {\"role\": \"user\", \"content\": f\"Tell me about the following title in 10 words: {title}\"},\n",
    "                        {\"role\": \"assistant\", \"content\": content},\n",
    "                    ]\n",
    "                    # Gera o texto formatado diretamente\n",
    "                    yield {\"text\": tokenizer.apply_chat_template(messages, tokenize=False)}\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "            \n",
    "verify_gpu_availability()\n",
    "\n",
    "# raw_data = read_json_file(solve_relative_path(\"datasets/trn.json\"))\n",
    "# formatted_data = format_data_for_training_with_phi35(raw_data)\n",
    "# dataset = Dataset.from_dict({\"text\": [item[\"text\"] for item in formatted_data]})\n",
    "dataset_path = solve_relative_path(\"datasets/trn.json\")\n",
    "dataset = Dataset.from_generator(format_data_generator, gen_kwargs={\"file_path\": dataset_path})\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir = \"/home/thiagofernandes101/projects/fiap/FineTunningTechChallenge/results\", # Pasta para salvar os resultados\n",
    "#     per_device_train_batch_size = 1, # Tamanho do lote. Use 1 ou 2 se tiver pouca VRAM.\n",
    "#     gradient_accumulation_steps = 8, # Acumula gradientes para simular um lote maior (2 * 4 = lote efetivo de 8)\n",
    "#     warmup_steps = 5, # Passos de aquecimento para o otimizador\n",
    "#     max_steps = 60, # N√∫mero total de passos de treinamento. Ajuste conforme necess√°rio.\n",
    "#     learning_rate = 2e-4, # Taxa de aprendizado\n",
    "#     fp16 = not torch.cuda.is_bf16_supported(), # Usa precis√£o de 16 bits\n",
    "#     bf16 = torch.cuda.is_bf16_supported(),\n",
    "#     logging_steps = 1, # A cada quantos passos ele mostra o progresso (loss)\n",
    "#     optim = \"adamw_8bit\", # Otimizador eficiente em mem√≥ria\n",
    "#     weight_decay = 0.01,\n",
    "#     lr_scheduler_type = \"linear\",\n",
    "#     seed = 42,\n",
    "#     packing = True, # Usa packing para aproveitar melhor a VRAM\n",
    "# )\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"/home/thiagofernandes101/projects/fiap/FineTunningTechChallenge/results\",\n",
    "    # ALTERA√á√ÉO 1: Reduzir o batch size para 1. ESSENCIAL!\n",
    "    per_device_train_batch_size = 1, \n",
    "    # ALTERA√á√ÉO 2: Aumentar a acumula√ß√£o de gradiente para compensar\n",
    "    gradient_accumulation_steps = 8,\n",
    "    warmup_steps = 5,\n",
    "    max_steps = 60,\n",
    "    learning_rate = 2e-4,\n",
    "    fp16 = not torch.cuda.is_bf16_supported(),\n",
    "    bf16 = torch.cuda.is_bf16_supported(),\n",
    "    logging_steps = 1,\n",
    "    optim = \"adamw_8bit\",\n",
    "    weight_decay = 0.01,\n",
    "    lr_scheduler_type = \"linear\",\n",
    "    seed = 42,\n",
    "    # Adicionar para liberar mem√≥ria ap√≥s cada etapa, pode ajudar\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=10, # Salva checkpoints a cada 10 passos, por exemplo\n",
    "    save_total_limit=2, # Mant√©m apenas os 2 √∫ltimos checkpoints\n",
    ")\n",
    "\n",
    "# trainer = SFTTrainer(\n",
    "#     model = model,\n",
    "#     tokenizer = tokenizer,\n",
    "#     train_dataset = dataset,\n",
    "#     dataset_text_field = \"text\",\n",
    "#     max_seq_length = max_seq_length,\n",
    "#     args = training_args,\n",
    "# )\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    args = training_args,\n",
    "    # ALTERA√á√ÉO 3: Adicionar packing para maior efici√™ncia\n",
    "    packing = True,\n",
    ")\n",
    "\n",
    "print(\"Iniciando o fine-tuning...\")\n",
    "trainer.train()\n",
    "print(\"Fine-tuning conclu√≠do!\")\n",
    "\n",
    "\n",
    "title_for_testing = \"A Day in the Life of China\"\n",
    "message_for_testing = [\n",
    "    {\"role\": \"user\", \"content\": f\"Tell me about the following title in 10 words: {title_for_testing}\"},\n",
    "]\n",
    "input_ids = tokenizer.apply_chat_template(message_for_testing, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(input_ids=input_ids, max_new_tokens=256, use_cache=True)\n",
    "results = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "\n",
    "print(\"\\n--- TESTE DE INFER√äNCIA ---\")\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
