{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b78a0772",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thiagofernandes101/projects/fiap/FineTunningTechChallenge/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from datasets import Dataset, Features, Value\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import re\n",
    "import html\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03a14c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 1024\n",
    "dtype = None\n",
    "load_in_4bit = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9deaa167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str):\n",
    "    text = html.unescape(text)\n",
    "    text = re.sub(r\"--.*\", \"\", text) # Remove lines starting with '--' or similar patterns\n",
    "    text = re.sub(r\"[^\\x00-\\x7F]+\", \"\", text) # Remove non-ASCII characters (optional, depending on your dataset)\n",
    "    text = re.sub(r\"\\s+\", \" \", text) # Remove excessive whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a98619e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_file(file_path: str):\n",
    "    training_data = []\n",
    "\n",
    "    with open(file_path, \"r\") as file:\n",
    "        buffer: str = \"\" \n",
    "        for line in file:\n",
    "            buffer += line.strip()\n",
    "            try:\n",
    "                item: dict = json.loads(buffer)\n",
    "                buffer: str = \"\"\n",
    "                title: str = clean_text(item.get(\"title\", \"\"))\n",
    "                content: str = clean_text(item.get(\"content\", \"\"))\n",
    "                if title and content:\n",
    "                    yield {\"title\": title, \"content\": content}\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb63a54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_for_training(example: dict, tokenizer) -> dict:\n",
    "    \"\"\"\n",
    "    Formats a title and content into a chat-like format using the tokenizer's chat template.\n",
    "    This teaches the model to respond with the book's content when asked about its title.\n",
    "    \"\"\"\n",
    "    conversation = [\n",
    "        {'role': 'user', 'content': f'Tell me about the book titled \"{example[\"title\"]}\". What is its content?'},\n",
    "        {'role': 'assistant', 'content': example['content']}\n",
    "    ]\n",
    "    formatted_text = tokenizer.apply_chat_template(conversation, tokenize=False, add_generation_prompt=False)\n",
    "    return {\"text\": formatted_text + tokenizer.eos_token}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bab92bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.9.5: Fast Llama patching. Transformers: 4.56.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060 Laptop GPU. Num GPUs = 1. Max memory: 6.0 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.9.5 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "fourbit_models = [\n",
    "    #\"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
    "    #\"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
    "    #\"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    #\"unsloth/gemma-2-9b-bnb-4bit\"\n",
    "]\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Phi-3.5-mini-instruct\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,\n",
    "    loftq_config = None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1652951",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1389915/1389915 [01:26<00:00, 16151.72 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"/home/thiagofernandes101/projects/fiap/FineTunningTechChallenge/datasets/trn.json\"\n",
    "\n",
    "raw_data = read_json_file(dataset_path)\n",
    "training_data_list = [{\"title\": item[\"title\"], \"content\": item[\"content\"]} for item in raw_data]\n",
    "base_dataset = Dataset.from_list(training_data_list)\n",
    "dataset = base_dataset.map(lambda x: format_for_training(x, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bdb4d83",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m      1\u001b[39m training_args = TrainingArguments(\n\u001b[32m      2\u001b[39m     output_dir=\u001b[33m\"\u001b[39m\u001b[33m/home/thiagofernandes101/projects/fiap/FineTunningTechChallenge/results\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      3\u001b[39m     per_device_train_batch_size=\u001b[32m2\u001b[39m,  \u001b[38;5;66;03m# Aumente se sua GPU permitir para acelerar\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m     save_total_limit=\u001b[32m2\u001b[39m,\n\u001b[32m     18\u001b[39m )\n\u001b[32m     20\u001b[39m trainer = SFTTrainer(\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     model = \u001b[43mmodel\u001b[49m,\n\u001b[32m     22\u001b[39m     tokenizer = tokenizer,\n\u001b[32m     23\u001b[39m     train_dataset = dataset,\n\u001b[32m     24\u001b[39m     dataset_text_field = \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     25\u001b[39m     max_seq_length = max_seq_length,\n\u001b[32m     26\u001b[39m     args = training_args,\n\u001b[32m     27\u001b[39m     packing = \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     28\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/home/thiagofernandes101/projects/fiap/FineTunningTechChallenge/results\",\n",
    "    per_device_train_batch_size=2,  # Aumente se sua GPU permitir para acelerar\n",
    "    gradient_accumulation_steps=4, # Ajuste para manter um lote efetivo de 8 (2*4)\n",
    "    warmup_steps=10,\n",
    "    # max_steps = 70, # Remova esta linha\n",
    "    num_train_epochs=1, # Adicione esta linha para treinar em todo o dataset uma vez\n",
    "    learning_rate=2e-4,\n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    logging_steps=1,\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=42,\n",
    "    save_strategy=\"epoch\", # Salvar ao final de cada Ã©poca\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    args = training_args,\n",
    "    packing = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfe6f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_for_testing = \"A Day in the Life of China\"\n",
    "message_for_testing = [\n",
    "    {\"role\": \"user\", \"content\": f\"Regarding the book {title_for_testing}, what was the authorâ€™s primary goal in writing it?\"},\n",
    "]\n",
    "input_ids = tokenizer.apply_chat_template(message_for_testing, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(input_ids=input_ids, max_new_tokens=256, use_cache=True)\n",
    "results = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "\n",
    "print(\"\\n--- TESTE ANTES DO TREINO ---\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d2414a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Iniciando o fine-tuning...\")\n",
    "trainer.train()\n",
    "print(\"Fine-tuning concluÃ­do!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "767e6e7a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m model\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m tokenizer\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[43mtrainer\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m dataset\n\u001b[32m      6\u001b[39m gc.collect()\n",
      "\u001b[31mNameError\u001b[39m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "del model\n",
    "del tokenizer\n",
    "del trainer\n",
    "del dataset\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606315ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_for_testing = \"A Day in the Life of China\"\n",
    "message_for_testing = [\n",
    "    {\"role\": \"user\", \"content\": f\"Regarding the book {title_for_testing}, what was the authorâ€™s primary goal in writing it?\"},\n",
    "]\n",
    "input_ids = tokenizer.apply_chat_template(message_for_testing, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(input_ids=input_ids, max_new_tokens=256, use_cache=True)\n",
    "results = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "\n",
    "print(\"\\n--- TESTE DE INFERÃŠNCIA ---\")\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
